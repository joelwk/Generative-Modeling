{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras import layers, models, losses, callbacks\n",
    "\n",
    "from generative_text.general_tnn_generative.process import main\n",
    "from generative_text.utils.fnProcessing import read_config\n",
    "\n",
    "config_params = read_config(section='params', config_path='./generative_text/configkeras.ini')\n",
    "config_clearml = read_config(section='clearml', config_path='./generative_text/configkeras.ini')\n",
    "\n",
    "'''\n",
    "Load dataframe with test column\n",
    "'''\n",
    "\n",
    "train_ds, val_ds, test_ds, combined_vocab = main(training_data, input_col='text', clean_col='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative_text.utils import fnContextPairing\n",
    "\n",
    "included_entity_labels = ['PERSON', 'PRODUCT', 'ORG', 'FAC', 'NORP']\n",
    "# Replace with your path to the dump file. Download here: https://dumps.wikimedia.org/other/wikibase/wikidatawiki/\n",
    "path_to_dump_file = './drive/MyDrive/research/chanscope/data/simplewiki-20211001-pages-articles-multistream.xml'\n",
    "\n",
    "context_pairing = ContextPairing(path_to_dump_file, included_entity_labels)\n",
    "context_pairing.run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. General Transformer Generative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative_text.general_generative_keras.tnn import TransformerBlock, TokenAndPositionEmbedding\n",
    "from generative_text.general_generative_keras.train import train_model, TrainTextGenerator, CustomSchedule\n",
    "from generative_text.general_generative_keras.evaluate import TextGenerator, CustomSchedule\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./generative_text/configkeras.ini')\n",
    "params = config[\"params\"]\n",
    "epochs = int(params['epochs']) \n",
    "\n",
    "LOAD_MODEL = False\n",
    "MODEL_PATH = './models/general_generative/model_1.h5'\n",
    "\n",
    "if LOAD_MODEL and os.path.exists(MODEL_PATH):\n",
    "    model = train_model(preload_model=True, model_path=MODEL_PATH)\n",
    "else:\n",
    "    model = train_model(preload_model=False, model_path=MODEL_PATH)\n",
    "\n",
    "def get_callbacks():\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath='./models/general_generative/weights.{epoch:02d}-{val_loss:.2f}.ckpt',\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',                                     \n",
    "        verbose=1\n",
    "    )\n",
    "    text_generator = TrainTextGenerator(index_to_word=combined_vocab)\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    return [model_checkpoint_callback, text_generator, early_stopping_callback]\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clearml import Task, Dataset as ClearMLDataset\n",
    "from generative_text.general_tnn_generative.process import main\n",
    "from generative_text.general_tnn_generative.utils.fnProcessing import read_config\n",
    "from generative_text.general_tnn_generative.utils.fnOps import ClearMLOps, ClearMLOpsTraining\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "config_params = read_config(section='params', config_path='./generative_text/configkeras.ini')\n",
    "clearml_params = read_config(section='clearml', config_path='./generative_text/configkeras.ini')\n",
    "clearml_project_name = clearml_params['clearml_project_name']\n",
    "clearml_model_id = clearml_params['clearml_model_id']\n",
    "clearml_output_uri = clearml_params['clearml_output_uri']\n",
    "model_name = clearml_params['model_name']\n",
    "model_path = clearml_params['model_path']\n",
    "\n",
    "clean_training = ClearMLOpsTraining(clearml_params,config_params)\n",
    "clean_ops = ClearMLOps(config_path='./generative_text/configkeras.ini')\n",
    "\n",
    "clean_ops.set_creds_connect()\n",
    "clean_ops.list_datasets(clearml_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'Training Test 3'\n",
    "model = clean_training.run_clearml_training_task('Training Test 3', dataset_id='df7aa04e1581485188eab3ee4df11ea1', training_data=None, vocab=None, load_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative_text.general_generative_keras.tnn import TransformerBlock, TokenAndPositionEmbedding\n",
    "from generative.general_generative_keras.evaluate import TextGenerator, CustomSchedule\n",
    "\n",
    "MODEL_PATH = './models/general_generative/model_1.h5'\n",
    "with custom_object_scope({'CustomSchedule': CustomSchedule, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding': TokenAndPositionEmbedding}):\n",
    "    model = load_model(MODEL_PATH)\n",
    "\n",
    "test_text_gen = TextGenerator(model=model, index_to_word=combined_vocab, top_k=15, generation_type='general', sampling_type='top_k')\n",
    "info = test_text_gen.generate(\"Today in the news\", max_tokens=50, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Custom GPT Generative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Fold\" in more replies data according to training performance and system constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_replies(sample_size=150000, raw_data_path='/content/drive/MyDrive/research/chanscope/data/replies_raw_2.csv', replies_path='/content/drive/MyDrive/research/chanscope/data/replies/replies.csv'):\n",
    "    # Read and sample the raw data\n",
    "    raw_data = pd.read_csv(raw_data_path).sample(sample_size)\n",
    "\n",
    "    # Prepare the data\n",
    "    prepared_data = prepare_data(raw_data)\n",
    "    thread_headers = prepared_data.dropna(subset=['text_clean','posted_comment'])[['thread_id', 'thread_header', 'posted_comment', 'posted_date_time']]\n",
    "\n",
    "    # Find and augment dialogs\n",
    "    new_replies = find_dialogs(thread_headers)\n",
    "    new_replies = augment_dialogs(new_replies, prepared_data)\n",
    "    new_replies = new_replies.dropna()\n",
    "\n",
    "    # Read the existing replies and append new ones\n",
    "    complete_replies = pd.read_csv(replies_path)\n",
    "    complete_replies = pd.concat([complete_replies, new_replies]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Save the updated replies\n",
    "    complete_replies.to_csv(replies_path, index=False)\n",
    "\n",
    "    # Remove the sampled data from the original dataset and save it\n",
    "    remaining_data = pd.read_csv(raw_data_path)\n",
    "    remaining_data = remaining_data.loc[~remaining_data.index.isin(raw_data.index)]\n",
    "    remaining_data.to_csv(raw_data_path, index=False)\n",
    "    return complete_replies, remaining_data\n",
    "\n",
    "remaining_data,complete_replies = update_replies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.fnProcessing import find_dialogs, augment_dialogs,view_shapes\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from generative_text.general_gpt_generative.preprocessing import DirectoryManager  \n",
    "from generative_text.general_gpt_generative.preprocessing import initialize_and_prepare  \n",
    "from generative_text.general_gpt_generative.processing import process_data\n",
    "from generative_text.general_gpt_generative.evaluate import plot_text_pair_distribution, count_tokens_and_lengths,plot_history\n",
    "import pandas as pd\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./generative_text/configcustom.ini')\n",
    "config_params = config['params']\n",
    "config_paths = config['paths']\n",
    "paths = {key: config_paths[key] for key in config_paths}\n",
    "base_directory = paths['metadata_path']\n",
    "params = {key: config_params[key] for key in config_params}\n",
    "max_len = int(params['max_len'])\n",
    "vocab_size = int(params['vocab_size'])\n",
    "embedding_dim = int(params['embedding_dim'])\n",
    "num_heads = int(params['n_heads'])\n",
    "num_layers = int(params['n_layers'])\n",
    "key_dim = int(params['key_dim'])\n",
    "ff_dim = int(params['feed_forward_dim'])\n",
    "dropout_rate = float(params['dropout'])\n",
    "warmup_steps = int(params['warmup_steps'])\n",
    "activation = params['activation']\n",
    "epoch = int(params['epochs'])\n",
    "\n",
    "data_path = '../replies.csv'\n",
    "replies = pd.read_csv(f'{data_path}').drop_duplicates().sample(50)\n",
    "\n",
    "train_ds, val_ds, test_ds, vectorizer, text_pairs  = process_data(replies, base_directory, params)\n",
    "\n",
    "def view_shapes(dataset):\n",
    "    for example_input, example_target in dataset.take(1):\n",
    "        print(f\"Input shape: {example_input.shape}\")\n",
    "view_shapes(train_ds)\n",
    "\n",
    "# count tokens\n",
    "comment_tokens, response_comment_tokens = set(), set()\n",
    "comment_maxlen, response_maxlen = 0, 0\n",
    "for comment, response in text_pairs:\n",
    "    comment_tok, response_tok = comment.split(), response.split()\n",
    "    comment_maxlen = max(comment_maxlen, len(comment_tok))\n",
    "    response_maxlen = max(response_maxlen, len(response_tok))\n",
    "    comment_tokens.update(comment_tok)\n",
    "    response_comment_tokens.update(response_tok)\n",
    "\n",
    "print(f\"Total Comment tokens: {len(comment_tokens)}\")\n",
    "print(f\"Total Response tokens: {len(response_comment_tokens)}\")\n",
    "print(f\"Max Comment length: {comment_maxlen}\")\n",
    "print(f\"Max Response length: {response_maxlen}\")\n",
    "print(f\"{len(text_pairs)} total pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative_text.general_gpt_generative.tnn import transformer, CustomSchedule, masked_loss, masked_accuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "model_path = './models/general_gpt_custom/'\n",
    "model_name = 'best_model_1'\n",
    "model_full_path = os.path.join(model_path, model_name)\n",
    "\n",
    "# Check if the model exists and load it; otherwise, create a new one\n",
    "if os.path.exists(model_full_path):\n",
    "    logging.info(f\"Loading model from {model_full_path}\")\n",
    "    transformer_model = tf.keras.models.load_model(model_full_path, custom_objects={\n",
    "        'masked_loss': masked_loss,\n",
    "        'masked_accuracy': masked_accuracy,\n",
    "        'CustomSchedule': CustomSchedule,\n",
    "        'PositionalEmbedding': PositionalEmbedding\n",
    "    })\n",
    "else:\n",
    "    logging.info(\"Initializing a new model.\")\n",
    "    transformer_model = transformer(\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        key_dim=key_dim,\n",
    "        ff_dim=ff_dim,\n",
    "        vocab_size=len(vectorizer.get_vocabulary()),\n",
    "        dropout=dropout_rate\n",
    "    )\n",
    "    # Define the learning rate schedule and optimizer\n",
    "    lr_schedule = CustomSchedule(key_dim, warmup_steps)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    # Compile the model\n",
    "    transformer_model.compile(\n",
    "        loss=masked_loss, \n",
    "        optimizer=optimizer, \n",
    "        metrics=[masked_accuracy]\n",
    "    )\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_path,'/weights.{epoch:02d}-{val_loss:.2f}.ckpt'), \n",
    "        monitor='val_loss', save_best_only=True, save_weights_only=False, save_format='tf')]\n",
    "\n",
    "# Train the model\n",
    "history = transformer_model.fit(\n",
    "    train_ds, \n",
    "    epochs=epoch, \n",
    "    validation_data=val_ds, \n",
    "    callbacks=callbacks)\n",
    "\n",
    "test_loss, test_accuracy = transformer_model.evaluate(test_ds)\n",
    "transformer_model.save(model_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(sequence, vectorizer):\n",
    "    vocab = vectorizer.get_vocabulary()\n",
    "    return \" \".join([vocab[i] for i in sequence if i < len(vocab)])\n",
    "\n",
    "def evaluate_translation(model, dataset, vectorizer, num_samples=5):\n",
    "    \"\"\"\n",
    "    Evaluate the translation performance of the model by generating responses\n",
    "    for a given number of samples from the dataset.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained GPT-style transformer model.\n",
    "    - dataset: A tf.data.Dataset object containing the input and target pairs.\n",
    "    - vectorizer: The TextVectorization layer used for tokenizing text.\n",
    "    - num_samples: Number of samples to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Select a few samples from the dataset\n",
    "    for input_text, _ in dataset.take(num_samples):\n",
    "        # Model prediction\n",
    "        predicted_sequence = model.predict(input_text)\n",
    "\n",
    "        # Decode the input text\n",
    "        input_text_decoded = decode_sequence(input_text[0].numpy(), vectorizer)\n",
    "\n",
    "        # Decode the predicted sequence\n",
    "        predicted_text = decode_sequence(np.argmax(predicted_sequence[0], axis=-1), vectorizer)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Input Text: \", input_text_decoded)\n",
    "        print(\"Generated Response: \", predicted_text)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example usage\n",
    "evaluate_translation(transformer_model, test_ds, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
