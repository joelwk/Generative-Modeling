{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras import layers, models, losses, callbacks\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import configparser\n",
    "import os\n",
    "from utils.process import main\n",
    "data = pd.read_csv('../sampled_data.csv').sample(25000)\n",
    "train_ds, val_ds, test_ds, combined_vocab = main(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. General Generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative_text.general_generative_keras.tnn import TransformerBlock, TokenAndPositionEmbedding\n",
    "from generative_text.general_generative_keras.train import train_model, TrainTextGenerator, CustomSchedule\n",
    "from generative_text.general_generative_keras.evaluate import TextGenerator, CustomSchedule\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./generative_text/configkeras.ini')\n",
    "params = config[\"params\"]\n",
    "epochs = int(params['epochs']) \n",
    "\n",
    "LOAD_MODEL = False\n",
    "MODEL_PATH = './models/general_generative/model_1.h5'\n",
    "\n",
    "if LOAD_MODEL and os.path.exists(MODEL_PATH):\n",
    "    model = train_model(preload_model=True, model_path=MODEL_PATH)\n",
    "else:\n",
    "    model = train_model(preload_model=False, model_path=MODEL_PATH)\n",
    "\n",
    "def get_callbacks():\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=\"./models/general_generative/weights.{epoch:02d}-{val_loss:.2f}.ckpt\",\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',                                     \n",
    "        verbose=1\n",
    "    )\n",
    "    text_generator = TrainTextGenerator(index_to_word=combined_vocab)\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    return [model_checkpoint_callback, text_generator, early_stopping_callback]\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative_text.general_generative_keras.tnn import TransformerBlock, TokenAndPositionEmbedding\n",
    "from generative.general_generative_keras.evaluate import TextGenerator, CustomSchedule\n",
    "\n",
    "MODEL_PATH = './models/general_generative/model_1.h5'\n",
    "with custom_object_scope({'CustomSchedule': CustomSchedule, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding': TokenAndPositionEmbedding}):\n",
    "    model = load_model(MODEL_PATH)\n",
    "\n",
    "test_text_gen = TextGenerator(model=model, index_to_word=combined_vocab, top_k=15, generation_type='general', sampling_type='top_k')\n",
    "info = test_text_gen.generate(\"Today in the news\", max_tokens=50, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Custom Generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_replies(sample_size=150000, raw_data_path='/content/drive/MyDrive/research/chanscope/data/replies_raw_2.csv', replies_path='/content/drive/MyDrive/research/chanscope/data/replies/replies.csv'):\n",
    "    # Read and sample the raw data\n",
    "    raw_data = pd.read_csv(raw_data_path).sample(sample_size)\n",
    "\n",
    "    # Prepare the data\n",
    "    prepared_data = prepare_data(raw_data)\n",
    "    thread_headers = prepared_data.dropna(subset=['text_clean','posted_comment'])[['thread_id', 'thread_header', 'posted_comment', 'posted_date_time']]\n",
    "\n",
    "    # Find and augment dialogs\n",
    "    new_replies = find_dialogs(thread_headers)\n",
    "    new_replies = augment_dialogs(new_replies, prepared_data)\n",
    "    new_replies = new_replies.dropna()\n",
    "\n",
    "    # Read the existing replies and append new ones\n",
    "    complete_replies = pd.read_csv(replies_path)\n",
    "    complete_replies = pd.concat([complete_replies, new_replies]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Save the updated replies\n",
    "    complete_replies.to_csv(replies_path, index=False)\n",
    "\n",
    "    # Remove the sampled data from the original dataset and save it\n",
    "    remaining_data = pd.read_csv(raw_data_path)\n",
    "    remaining_data = remaining_data.loc[~remaining_data.index.isin(raw_data.index)]\n",
    "    remaining_data.to_csv(raw_data_path, index=False)\n",
    "    return complete_replies, remaining_data\n",
    "\n",
    "# Run the function\n",
    "remaining_data,complete_replies = update_replies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.fnProcessing import find_dialogs, augment_dialogs,view_shapes\n",
    "\n",
    "import tensorflow as tf\n",
    "from generative_text.general_chat_custom.preprocessing import DirectoryManager  \n",
    "from generative_text.general_chat_custom.preprocessing import initialize_and_prepare  \n",
    "from generative_text.general_chat_custom.processing import process_and_load_data\n",
    "from generative_text.general_chat_custom.evaluate import plot_text_pair_distribution, count_tokens_and_lengths,plot_history\n",
    "import pandas as pd\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./generative_text/configcustom.ini')\n",
    "config_params = config['params']\n",
    "config_paths = config['paths']\n",
    "paths = {key: config_paths[key] for key in config_paths}\n",
    "base_directory = paths['metadata_path']\n",
    "params = {key: config_params[key] for key in config_params}\n",
    "max_len = int(params['max_len'])\n",
    "vocab_size = int(params['vocab_size'])\n",
    "embedding_dim = int(params['embedding_dim'])\n",
    "num_heads = int(params['n_heads'])\n",
    "num_layers = int(params['n_layers'])\n",
    "key_dim = int(params['key_dim'])\n",
    "ff_dim = int(params['feed_forward_dim'])\n",
    "dropout_rate = float(params['dropout'])\n",
    "warmup_steps = int(params['warmup_steps'])\n",
    "activation = params['activation']\n",
    "epoch = int(params['epochs'])\n",
    "\n",
    "data_path = '../replies.csv'\n",
    "replies = pd.read_csv(f'{data_path}').drop_duplicates().sample(2500)\n",
    "config_directories = DirectoryManager.generate_config(base_directory)\n",
    "DirectoryManager.create_directories(config_directories) \n",
    "# Load supporting data\n",
    "text_pairs, voc_comment, voc_response_comment = initialize_and_prepare(base_directory, replies)\n",
    "\n",
    "# Load data\n",
    "train_ds, val_ds, test_ds, comment_vectorizer, response_comment_vectorizer  = process_and_load_data(replies)\n",
    "comment_tokens, response_comment_tokens, comment_maxlen, response_maxlen = count_tokens_and_lengths(text_pairs)\n",
    "view_shapes(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative_text.general_chat_custom.tnn import transformer, masked_loss, masked_accuracy\n",
    "from generative_text.general_chat_custom.tnn import CustomSchedule\n",
    "from generative_text.general_chat_custom.PositionalEmbedding import PositionalEmbedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "model_path = './models/'\n",
    "model_name = 'best_model.h5'\n",
    "model_full_path = os.path.join(model_path, model_name)\n",
    "\n",
    "def get_callbacks(model_path, model_name, patience=5):\n",
    "    early_stopping = EarlyStopping(monitor='val_masked_accuracy', patience=patience, restore_best_weights=False)\n",
    "    model_checkpoint = ModelCheckpoint(filepath=os.path.join(model_path, model_name), monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "    return [early_stopping, model_checkpoint]\n",
    "\n",
    "# Load or create model\n",
    "if os.path.isfile(model_full_path):\n",
    "    print(\"Best model checkpoint found. Loading...\")\n",
    "    transformer_model = tf.keras.models.load_model(model_full_path, custom_objects={\n",
    "        'masked_loss': masked_loss,\n",
    "        'masked_accuracy': masked_accuracy,\n",
    "        'CustomSchedule': CustomSchedule,\n",
    "        'PositionalEmbedding': PositionalEmbedding\n",
    "    })\n",
    "else:\n",
    "    print(\"No model checkpoint found. Creating a new one.\")\n",
    "    transformer_model = transformer(\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        key_dim=key_dim,\n",
    "        ff_dim=ff_dim,\n",
    "        vocab_size_src=len(comment_tokens),\n",
    "        vocab_size_tgt=len(response_comment_tokens),\n",
    "        dropout=dropout_rate\n",
    "    )\n",
    "\n",
    "# Compile model\n",
    "lr_schedule = CustomSchedule(key_dim, warmup_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "transformer_model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
    "# Train model\n",
    "callbacks = get_callbacks(model_path, model_name)\n",
    "history = transformer_model.fit(train_ds, epochs=epoch, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from generative_text.general_chat_custom.tnn import transformer, masked_loss, masked_accuracy\n",
    "\n",
    "from generative_text.general_chat_custom.tnn import CustomSchedule\n",
    "from generative_text.general_chat_custom.PositionalEmbedding import PositionalEmbedding\n",
    "\n",
    "custom_objects = {\n",
    "    \"PositionalEmbedding\": PositionalEmbedding,\n",
    "    \"CustomSchedule\": CustomSchedule,\n",
    "    \"masked_loss\": masked_loss,\n",
    "    \"masked_accuracy\": masked_accuracy\n",
    "}\n",
    "\n",
    "best_model_path = \"./models/best_model.h5\"\n",
    "\n",
    "# Load the trained model with custom objects\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Loading best model from {best_model_path}\")\n",
    "    with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "else:\n",
    "    print(\"Best model not found. Please check the path or train the model.\")\n",
    "\n",
    "# Define the translate function\n",
    "def translate(sentence, max_len=max_len):\n",
    "    \"\"\"Create the translated sentence.\"\"\"\n",
    "    # Tokenize the input sentence\n",
    "    enc_tokens = comment_vectorizer([sentence])\n",
    "    enc_tokens = tf.reshape(enc_tokens, (1, -1)) \n",
    "    lookup = list(response_comment_vectorizer.get_vocabulary())\n",
    "    start_sentinel, end_sentinel = \"[start]\", \"[end]\"\n",
    "    output_sentence = [start_sentinel]\n",
    "\n",
    "    # Generate the output sentence\n",
    "    for i in range(max_len):\n",
    "        vector = comment_vectorizer([\" \".join(output_sentence)])\n",
    "        dec_tokens = tf.reshape(vector[:, :-1], (1, -1))\n",
    "        pred = model([enc_tokens, dec_tokens])\n",
    "        # Debugging: Check the shape of the prediction tensor\n",
    "        print(f\"Prediction shape: {pred.shape}\")\n",
    "        if i >= pred.shape[1]:\n",
    "            print(f\"Index {i} is out of bounds for prediction with shape {pred.shape}.\")\n",
    "            break\n",
    "        word_index = tf.argmax(pred[0, i, :], axis=-1).numpy()\n",
    "        word = lookup[word_index]\n",
    "        output_sentence.append(word)\n",
    "        if word == end_sentinel:\n",
    "            break\n",
    "    return \" \".join(output_sentence[1:-1])  # Exclude start and end sentinels\n",
    "\n",
    "# Test the translate function\n",
    "test_count = 5\n",
    "for n in range(test_count):\n",
    "    thread, comment = random.choice(text_pairs)\n",
    "    translated = translate(thread)\n",
    "    print(f\"Test {n+1}:\")\n",
    "    print(f\"Thread: {thread}\")\n",
    "    print(f\"Expected Comment: {comment}\")\n",
    "    print(f\"Translated Comment: {translated}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
